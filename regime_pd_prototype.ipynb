{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb8ff5c",
   "metadata": {},
   "source": [
    "# Regime-Aware Credit PD Pipeline\n",
    "\n",
    "This notebook follows step-by-step instructions from README.md to build a regime-aware credit probability of default (PD) modeling pipeline.\n",
    "\n",
    "## Outline\n",
    "1. Read README.md File\n",
    "2. Parse Instructions from README.md\n",
    "3. Execute Instructions Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Read README.md File\n",
    "with open('README.md', 'r', encoding='utf-8') as f:\n",
    "    readme_content = f.read()\n",
    "print(readme_content[:1000])  # Print first 1000 characters for preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8faf323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Parse Instructions from README.md\n",
    "import re\n",
    "\n",
    "# Extract step instructions from README.md\n",
    "steps = re.findall(r'\\u{1F4C8}|\\u{1F4C9}|\\u{1F4C7}|\\u{1F4C6}|\\u{1F4C5}|\\u{1F4C4}|\\u{1F4C3}|\\u{1F4C2}|\\u{1F4C1}|\\u{1F4C0}|STEP \\d+.*?(?=\\n\\n|\\n\\u{1F4C8}|\\n\\u{1F4C9}|\\n\\u{1F4C7}|\\n\\u{1F4C6}|\\n\\u{1F4C5}|\\n\\u{1F4C4}|\\n\\u{1F4C3}|\\n\\u{1F4C2}|\\n\\u{1F4C1}|\\n\\u{1F4C0}|$)', readme_content, re.DOTALL)\n",
    "\n",
    "if not steps:\n",
    "    # Fallback: split by 'STEP' if emoji fails\n",
    "    steps = re.split(r'(?=STEP \\d+)', readme_content)\n",
    "\n",
    "for i, step in enumerate(steps):\n",
    "    print(f\"\\n--- Step {i+1} ---\\n{step[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c82af",
   "metadata": {},
   "source": [
    "## Section 3: Execute Instructions Programmatically\n",
    "\n",
    "The following cells will implement each step from the README.md instructions, including environment setup, data loading, HMM fitting, merging, modeling, calibration, and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0 — Setup Environment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "import xgboost as xgb\n",
    "\n",
    "print('Environment setup complete. Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 — Load Borrower Dataset\n",
    "# For prototype, generate synthetic borrower data\n",
    "np.random.seed(42)\n",
    "num_records = 1000\n",
    "dates = pd.date_range('2015-01-01', periods=num_records, freq='M')\n",
    "\n",
    "borrower_df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'leverage': np.random.normal(2, 0.5, num_records),\n",
    "    'interest_coverage': np.random.normal(5, 1.5, num_records),\n",
    "    'asset_growth': np.random.normal(0.05, 0.02, num_records),\n",
    "    'default': np.random.binomial(1, 0.08, num_records)\n",
    "})\n",
    "\n",
    "borrower_df['date'] = pd.to_datetime(borrower_df['date'])\n",
    "borrower_df = borrower_df.sort_values('date')\n",
    "print(borrower_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84709344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 — Create Synthetic Macro Data\n",
    "macro_dates = borrower_df['date'].unique()\n",
    "macro_df = pd.DataFrame({'date': macro_dates})\n",
    "\n",
    "# Create regime periods\n",
    "regime_periods = [\n",
    "    (0, 300, {'gdp_growth': 0.03, 'unemployment': 0.04, 'interest_rate': 0.03}),  # strong growth\n",
    "    (300, 700, {'gdp_growth': 0.02, 'unemployment': 0.06, 'interest_rate': 0.04}), # moderate\n",
    "    (700, 1000, {'gdp_growth': 0.01, 'unemployment': 0.08, 'interest_rate': 0.05}) # low growth\n",
    "]\n",
    "\n",
    "macro_df['gdp_growth'] = 0.0\n",
    "macro_df['unemployment'] = 0.0\n",
    "macro_df['interest_rate'] = 0.0\n",
    "\n",
    "for start, end, values in regime_periods:\n",
    "    macro_df.loc[start:end, 'gdp_growth'] = np.random.normal(values['gdp_growth'], 0.005, end-start+1)\n",
    "    macro_df.loc[start:end, 'unemployment'] = np.random.normal(values['unemployment'], 0.01, end-start+1)\n",
    "    macro_df.loc[start:end, 'interest_rate'] = np.random.normal(values['interest_rate'], 0.005, end-start+1)\n",
    "\n",
    "print(macro_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 — Fit Hidden Markov Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "macro_vars = ['gdp_growth', 'unemployment', 'interest_rate']\n",
    "scaler = StandardScaler()\n",
    "macro_scaled = scaler.fit_transform(macro_df[macro_vars])\n",
    "\n",
    "hmm = GaussianHMM(n_components=3, covariance_type='full', random_state=42)\n",
    "hmm.fit(macro_scaled)\n",
    "macro_df['regime'] = hmm.predict(macro_scaled)\n",
    "\n",
    "print('Transition matrix:')\n",
    "print(hmm.transmat_)\n",
    "\n",
    "print('Average macro values per regime:')\n",
    "print(macro_df.groupby('regime')[macro_vars].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfe415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 — Merge Regime with Borrower Data\n",
    "borrower_df = borrower_df.merge(macro_df[['date', 'regime']], on='date', how='left')\n",
    "print(borrower_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 — Train Baseline PD Model (No Regime)\n",
    "features = ['leverage', 'interest_coverage', 'asset_growth']\n",
    "\n",
    "# Split by date\n",
    "split_idx = int(0.7 * len(borrower_df))\n",
    "train_df = borrower_df.iloc[:split_idx]\n",
    "test_df = borrower_df.iloc[split_idx:]\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['default']\n",
    "X_test = test_df[features]\n",
    "y_test = test_df['default']\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "pred_proba_lr = lr.predict_proba(X_test)[:, 1]\n",
    "pred_lr = lr.predict(X_test)\n",
    "\n",
    "auc_lr = roc_auc_score(y_test, pred_proba_lr)\n",
    "cm_lr = confusion_matrix(y_test, pred_lr)\n",
    "\n",
    "print(f'Baseline Logistic Regression AUC: {auc_lr:.3f}')\n",
    "print('Confusion Matrix:')\n",
    "print(cm_lr)\n",
    "\n",
    "# Calibration check\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(pred_proba_lr, y_test, alpha=0.2)\n",
    "plt.xlabel('Predicted PD')\n",
    "plt.ylabel('Actual Default')\n",
    "plt.title('Default vs Predicted PD (Baseline)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6 — Train Regime-Aware PD Model (XGBoost)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "regime_encoder = LabelEncoder()\n",
    "borrower_df['regime_cat'] = regime_encoder.fit_transform(borrower_df['regime'])\n",
    "\n",
    "features_regime = features + ['regime_cat']\n",
    "X_train_regime = train_df[features_regime]\n",
    "X_test_regime = test_df[features_regime]\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(max_depth=3, learning_rate=0.05, n_estimators=100, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_regime, y_train)\n",
    "pred_proba_xgb = xgb_model.predict_proba(X_test_regime)[:, 1]\n",
    "pred_xgb = xgb_model.predict(X_test_regime)\n",
    "\n",
    "auc_xgb = roc_auc_score(y_test, pred_proba_xgb)\n",
    "print(f'Regime-Aware XGBoost AUC: {auc_xgb:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7 — Add Monotonic Constraints\n",
    "monotone_constraints = [1, -1, 0, 0]  # leverage (increasing), interest_coverage (decreasing), asset_growth (none), regime_cat (none)\n",
    "\n",
    "xgb_monotone = xgb.XGBClassifier(max_depth=3, learning_rate=0.05, n_estimators=100, monotone_constraints=monotone_constraints, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_monotone.fit(X_train_regime, y_train)\n",
    "pred_proba_xgb_monotone = xgb_monotone.predict_proba(X_test_regime)[:, 1]\n",
    "auc_xgb_monotone = roc_auc_score(y_test, pred_proba_xgb_monotone)\n",
    "print(f'Monotone XGBoost AUC: {auc_xgb_monotone:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8 — Probability Calibration\n",
    "calibrator = CalibratedClassifierCV(xgb_monotone, method='isotonic', cv='prefit')\n",
    "calibrator.fit(X_train_regime, y_train)\n",
    "pred_proba_calibrated = calibrator.predict_proba(X_test_regime)[:, 1]\n",
    "\n",
    "# Calibration curve\n",
    "plt.figure(figsize=(6,4))\n",
    "prob_true, prob_pred = calibration_curve(y_test, pred_proba_calibrated, n_bins=10)\n",
    "plt.plot(prob_pred, prob_true, marker='o', label='Calibrated')\n",
    "prob_true_raw, prob_pred_raw = calibration_curve(y_test, pred_proba_xgb_monotone, n_bins=10)\n",
    "plt.plot(prob_pred_raw, prob_true_raw, marker='x', label='Raw')\n",
    "plt.xlabel('Predicted PD')\n",
    "plt.ylabel('Actual Default Rate')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "auc_calibrated = roc_auc_score(y_test, pred_proba_calibrated)\n",
    "print(f'Calibrated XGBoost AUC: {auc_calibrated:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9 — Compare Results\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Baseline Logistic', 'XGBoost', 'Calibrated XGBoost'],\n",
    "    'AUC': [auc_lr, auc_xgb, auc_calibrated],\n",
    "    'Comments': ['No regime', 'With regime', 'With regime + calibration']\n",
    "})\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 10 — Print Insights\n",
    "print('Regime transition matrix:')\n",
    "print(hmm.transmat_)\n",
    "\n",
    "print('Default rate per regime:')\n",
    "default_rate_per_regime = borrower_df.groupby('regime')['default'].mean()\n",
    "print(default_rate_per_regime)\n",
    "\n",
    "print('AUC comparison:')\n",
    "print(results)\n",
    "\n",
    "print('Feature importance from XGBoost:')\n",
    "importances = xgb_model.feature_importances_\n",
    "for feat, imp in zip(features_regime, importances):\n",
    "    print(f'{feat}: {imp:.3f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
